\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[
top=2cm,
bottom=2cm,
left=3cm,
right=2cm,
headheight=17pt, % as per the warning by fancyhdr
includehead,includefoot,
heightrounded, % to avoid spurious underfull messages
]{geometry} 
\geometry{a4paper}
\usepackage[ngerman]{babel}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[table]{xcolor}
\usepackage{diagbox}
\usepackage{lipsum}
\usepackage{pgfplots}
\usepackage{booktabs}

\lstMakeShortInline[columns=fixed]|

% Assembler
\lstdefinelanguage
{Assembler} % based on the "x86masm" dialect
% with these extra keywords:
{morekeywords={call, mov}} % etc.

% Lecture Name, exercise number, group number/members
\newcommand{\lecture}{Parallel Computer Architecture}
\newcommand{\exercise}{Exercise 4}
\newcommand{\groupnumber}{Group 04}
\newcommand{\groupmembersshort}{Barley, Barth, Nisblé}
\newcommand{\groupmemberslist}{Barley, Daniel\\Barth, Alexander\\Nisblé, Patrick}
\newcommand{\duedate}{2019-11-29, 14:00}

\fancyhf{}
\fancyhead[L]{\groupnumber}
\fancyhead[R]{\textsc{\groupmembersshort}}
\fancyfoot[C]{\lecture: \exercise}
\fancyfoot[R] {\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\pagestyle{fancy}

\begin{document}
	\begin{titlepage}
		\centering
		
		{\scshape\LARGE Universität Heidelberg\\Institute for Computer Engineering (ZITI) \par}
		\vspace{1.5cm}
		{\scshape\Large Master of Science Computer Engineering \par}
		\vspace{0.5cm}
		{\scshape\Large \lecture \par}
		\vspace{1.5cm}
		{\huge\bfseries \exercise \par}
		\vspace{2cm}
		{\Large \groupnumber \itshape  \\ \groupmemberslist \par}
		\vfill
		
		
		% Bottom of the page
		{\large Due date \duedate \par}
	\end{titlepage}

\setcounter{section}{4}
\subsection{Numerische Integration revisited}

\subsubsection{Parallele Implementierung mittels PThreads}

Aufgrund der Assoziativität der Summe kann diese in beliebiger Reihenfolge ausgeführt werden.
Wir teilen deshalb jedem Thread eine Partialsumme zu.
Dazu wird die Gesamtzahl and Iterationen durch die Anzahl Threads geteilt (Ceil Division, überschüßige Iterationen werden später durch Minimum ausgefiltert).
Geht die Division nicht auf arbeitet der Letzte Thread weniger Elemente ab.
Somit wird sichergestellt, dass alle Threads etwa gleich lange zur Fertigstellung brauchen.
Da im Kern Schleife keine Speicherzugriffe stattfinden ist die Aufteilung der Summen frei wählbar.
Die implementierte Aufteilung wurde der Simplizität wegen verwendet.
Die einzelnen Partialsummen werden am Ende zusammengeführt.
Da die Threads auf keine gemeinsamen Speicherstrukturen zugreifen gibt es während der Ausführungszeit keine
Notwendigkeit zur Synchronisation, erst wenn die Partialsummen aufaddiert werden.
Dies geschiet hinter der Thread join Barrier wird also erst ausgeführt wenn der letzte Thread beended wurde.
Alternativ zur Reduktion aus einem Ergebnissarray hätte auch ein \texttt{atomic\_int} verwendet werden können,
da aber nur sehr wenige Zwischenergebnisse anfallen führt das interne locking welches damit verbunden ist zu 
schlechterer Performanz.

\subsubsection{Experimente und Evaluation}

\noindent \textbf{a.}

Die Auflösung des \texttt{time} commands ist zu klein um die Wallclock zu messen.
\begin{table}[htbp]
	\centering
	\caption[Ausführungszeit $t_{compute}$ (\si{\second})]{Ausführungszeit $t_{compute}$ (\si{\milli\second})}
	\begin{tabular}{c|l|l|l|l|l|l}
		\hline
		\cellcolor{gray!40}\textbf{\diagbox{n}{Threads}} & \multicolumn{1}{c}{\cellcolor{gray!40}\textbf{1}} & \multicolumn{1}{c}{\cellcolor{gray!40}\textbf{2}} & \multicolumn{1}{c}{\cellcolor{gray!40}\textbf{4}} &
		\multicolumn{1}{c}{\cellcolor{gray!40}\textbf{8}} &
		\multicolumn{1}{c}{\cellcolor{gray!40}\textbf{16}} &
		\multicolumn{1}{c}{\cellcolor{gray!40}\textbf{32}} \\
		\hline\hline
		2 & 0.23  & 0.257 & 0.321 & 0.495 & 1.496 & 1.866 \\
		3 & 0.239 & 0.28  & 0.322 & 0.434 & 1.413 & 1.94  \\
		4 & 0.294 & 0.318 & 0.338 & 0.531 & 1.507 & 1.953 \\
		5 & 0.919 & 0.641 & 0.475 & 0.522 & 1.512 & 1.915 \\
		6 & 7.179 & 3.731 & 2.039 & 1.859 & 1.322 & 2.149 \\
	\end{tabular}
	\label{tab:tcomp}
\end{table}

%\begin{table}[h]
	%\centering
	%\caption[Ausführungszeit $t_{wall}$ (\si{\second})]{Ausführungszeit $t_{wall}$ (\si{\milli\second})}
	%\begin{tabular}{c|l|l|l|l|l}
		%\hline
		%\cellcolor{gray!40}\textbf{\diagbox{Threads}{n}} & \multicolumn{1}{c}{\cellcolor{gray!40}\textbf{2}} & \multicolumn{1}{c}{\cellcolor{gray!40}\textbf{3}} & \multicolumn{1}{c}{\cellcolor{gray!40}\textbf{4}} &
		%\multicolumn{1}{c}{\cellcolor{gray!40}\textbf{5}} &
		%\multicolumn{1}{c}{\cellcolor{gray!40}\textbf{6}} \\
		%\hline\hline
		%1 &  &  & & & \\\hline
		%2 &  &  & & & \\\hline
		%4 &  &  & & & \\\hline
		%8 &  &  & & & \\\hline
		%16 &  &  & & & \\\hline
		%32 &  &  & & & \\\hline
	%\end{tabular}
	%\label{tab:twall}
%\end{table}

\begin{figure}[htpb]
	\centering
	\input{exec_time.pgf}
	\caption{$ t_{\textrm{compute}} $ für verschiedene Problemgrößen, aufgrund der schlechten auflösung von \texttt{gettimeofday()} unterliegen die Wert für sehr kleine Problemgrößen starken Schwankungen}%
	\label{fig:exec}
\end{figure}

\begin{figure}[htpb]
	\centering
	\input{speedup.pgf}
	\caption{Speedups für verschiedene Problemgrößen, der Overhead der durch Threaderstellung ensteht ist besonders bei kleinen Größen zu sehen, als größter Speedup ist ein Speedup von 5 zu verzeichnen, bei 16 Threads und $ 10^6 $ Iterationen}%
	\label{fig:speedup}
\end{figure}

\noindent \textbf{b.}

Folgende Werte wurden für $ \pi $ berechnet (siehe Tabelle~\ref{tab:pi_approx}).
Wir erhalten die gleichen Werte für alle Anzahlen an Threads:
(Innerhalb der Fehlertoleranz, da Floatingpoint Operationen nicht kommutativ)

\begin{table}[htbp]
	\centering
	\caption{Approximierte Werte fuer \pi}
	\label{tab:pi_approx}
	\begin{tabular}{ccc}
	\toprule
		& \multicolumn{2}{c}{Values} \\
	\cmidrule(r){2-3}
		Iterations $\log_{10}$ & $\pi$    & $\Delta$   \\
	\midrule
		2 & 3.1416009869231227    & \SI{8.33333e-06}{} \\
		3 &    3.1415927369231298     & \SI{8.33333e-08}{} \\
		4 & 3.1415926544232335     & \SI{8.3344e-10}{} \\
		5 & 3.1415926535989005      & \SI{9.10738e-12}{} \\
		6 & 3.1415926535913021      & \SI{1.50902e-12}{} \\
	\bottomrule
	\end{tabular}
\end{table}

\noindent \textbf{c.}

Die erreichten Speedups sind in Abbildung~\ref{fig:speedup} zu sehen.
Es ist deutlich zu sehen, dass bei kleinen Problemgrößen der Overhead durch
threading das Program stark verlangsamt.
Ab einer Problemgröße von $ 10^5 $ sind positive Speedups zu erreichen, jedoch nur
bis 8 Threads mit dem maximalen Speedup von ca. 2 bei 4 threads.
Für $ 10^6 $ erreichen wir einen Speedup von ca. 5.3 bei 16 Threads, danach überwiegt wieder der
Overhead.

\subsection{Prozesse vs. Threads}

\noindent \textbf{a.}

Prozesse liefern die Ressourcen, welche für die Ausführung eines Programms erforderlich sind.Ein Prozess hat einen virtuellen Adressraum, ausführbaren Code, offene Handles zu Systemobjekten, einen eindeutigen Prozessidentifier, eine Prioritätsklasse, minimaler und maximaler Arbeitsbedarf und mindestens einen Ausführungsthread. Jeder Prozess startet mit einem einzelnen Thread und kann zusätzliche Threads erstellen.

Ein Thread ist ein Objekt innerhalb eines Prozesses, das zeitlich festgelegt ausgeführt werden kann. Alle Threads eines Prozesses teilen sich den virtuellen Adressraum und Systemressourcen, besitzen Scheduling Prioritäten, lokalen Speicher, einen eindeutigen Threadidentifier und einen Strukturensatz, der den Thread Kontext speichert, bis die Ausführung des Threads festgelegt ist. Der Kontext beinhaltet den Maschinenregistersatz des Threads, den Kernel-Stack, einen Umgebungsblock und einen Benutzer-Stack im Adressraum des übergeordneten Prozesses.

\noindent \textbf{b.}

Die Kommunikation zwischen Threads ist programmiertechnisch einfacher als die zwischen mehrerer Prozesse. Kontextwechsel zwischen Threads sind schneller als Prozesswechsel. Das Betriebssystem kann Threads schneller stoppen und einen anderen starten, als mit zwei Prozessen.

\subsection{Klassifikation nach Flynn}

\noindent \textbf{a.}

Die Zuordnung zur Klasse der MISD-Systeme ist schwierig, da mit einem Datensatz mehrere Funktionseinheiten unterschiedliche Operationen durchführen. Genau genommen unterscheiden sich die Daten somit nach der Durchführung. Des Weiteren sind sie weniger verbreitet als MIMD- und SIMD-Systeme, welche geeigneter sind für übliche parallele Datentechniken.

\noindent \textbf{b.}

Ein Vektorrechner bearbeitet quasi-parallel mehrere Daten durch Pipelining. Dabei werden Maschinenbefehle in Teilaufgaben zerlegt. Diese Teilaufgaben werden für mehrere Befehle parallel ausgeführt.

Beim Feldrechner berechnen mehrere Recheneinheiten parallel die gleiche Operation auf verschiedenen Daten.

\end{document}
